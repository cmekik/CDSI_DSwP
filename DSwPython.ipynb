{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science with Python (Pandas)\n",
    "\n",
    "Can Åžerif Mekik\n",
    "\n",
    "PhD Candidate <br/>\n",
    "Department of Cognitive Science <br/>\n",
    "Rensselaer Polytechnic Institute\n",
    "\n",
    "December 6, 2021\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<td><img src=CDSI_Fac.of.Sc_logo.png alt=\"CDSI Logo\" width=\"300\"/></td>\n",
    "<td><img src=mcgill_ccr_approval_croppedforblock_0.png alt=\"CCR Approved Logo\" width=\"300\"/></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introductory Remarks\n",
    "\n",
    "This workshop assumes minimal working knowledge of Python. \n",
    "\n",
    "Pandas is a great place to start using Python for data science\n",
    "- Similar feel to other stats software like R or Stata\n",
    "- Works well on its own but also integrates well with the Python data science ecosystem\n",
    "\n",
    "`Pandas` is excellent for [Data Wrangling](https://en.wikipedia.org/wiki/Data_wrangling), our main topic. \n",
    "\n",
    "This is the process making raw data ready for statistical analysis and/or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Useful Resources\n",
    "\n",
    "The [Pandas Cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) is an excellent two-page summary of essential `pandas` features.\n",
    "\n",
    "The [Official Pandas Docs](https://pandas.pydata.org/docs/) are the single best resource for information on `pandas` short of the source code.\n",
    "\n",
    "If you are looking to learn about a specific function or feature, go into the API section.\n",
    "\n",
    "The docs also offer tutorials and other helpful material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup\n",
    "2. Main Data Structures\n",
    "3. Basic Data Wrangling: Loading, Viewing, Cleaning, and Enriching Data\n",
    "4. More Data Wrangling: Aggregating, Reshaping, Merging and Concatenating Data\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "We will walk through the first steps of analyzing a sample dataset.\n",
    "\n",
    "To follow the workshop on your own machine, you should have Anaconda already installed.\n",
    "\n",
    "https://www.anaconda.com/products/individual\n",
    "\n",
    "This will automatically include the necessary dependencies.\n",
    "\n",
    "Our data set is Semra Sevi's Canadian Federal Elections dataset.\n",
    "\n",
    "You can find a copies of the dataset and code at the following addresses.\n",
    "\n",
    "- Code: https://github.com/cmekik/CDSI_DSwP\n",
    "- Data: https://doi.org/10.7910/DVN/ABFNSQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Getting Ready to Code\n",
    "\n",
    "`Jupyter` is a python tool for rich interactive coding that ships with Anaconda.\n",
    "\n",
    "This presentation uses `Jupyter` notebook, in fact!\n",
    "\n",
    "To get set, create a new folder in which you will work and copy the materials into it.\n",
    "\n",
    "Then launch your machines console, navigate to your folder, activate your conda environment, and run the following.\n",
    "\n",
    "```jupyter notebook```\n",
    "\n",
    "This should launch Jupyter notebook in your browser. When it does, you can open the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Installing and Importing pandas\n",
    "\n",
    "`pandas` comes pre-packaged in Anaconda.\n",
    "\n",
    "You can always install it using the following pip command: ```pip install pandas```\n",
    "\n",
    "If you have conda, but not pandas, you can also do: ```conda install pandas```\n",
    "\n",
    "Base `pandas` has only a few dependencies, but you may have to install optional dependencies depending on your work and your setup.\n",
    "\n",
    "For a full list of optional dependencies see the\n",
    "[Installation Documentation](https://pandas.pydata.org/docs/getting_started/install.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import the pandas library!\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main Data Structures\n",
    "\n",
    "There are two main data structures in `pandas`. \n",
    "- `Series` represent one single column of data\n",
    "- `DataFrame` represents a two dimensional array of data\n",
    "\n",
    "These are *array-like* data structures. \n",
    "\n",
    "- They have fixed dimensions.\n",
    "- Their entries are of a homogeneous datatype.\n",
    "- They are associated with indices, which help with data access\n",
    "- They support a variety of mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas Series\n",
    "\n",
    "> Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2, 3, 4, 5], index=[\"Lbl1\", \"Lbl2\", \"Lbl3\", \"Lbl4\", \"Lbl5\"])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s.shape, s.size, s.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Series Data Access\n",
    "\n",
    "There are *many* ways to access data with Series objects.\n",
    "\n",
    "Which one you use will depend on the situation.\n",
    "\n",
    "Let's look at some common patterns.\n",
    "\n",
    "See the docs on [Indexing and Selecting Data](https://pandas.pydata.org/docs/user_guide/indexing.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using `loc` and `iloc`\n",
    "\n",
    "These are the basic data access methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s.loc[\"Lbl1\"]  # access data by index label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.iloc[0] # access data by position in index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Using Regular Subscripts\n",
    "\n",
    "Subscripts try to behave intelligently depending on the data type you pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s[\"Lbl1\"], s[0] \n",
    "# direct indexing; multifunction, tries to be smart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Subsetting Data\n",
    "\n",
    "We can use boolean/logical expressions to select data.\n",
    "\n",
    "See the cheat sheet for a quick list of different operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 < s # Constructing a boolean series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[2 < s] # Simple boolean query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[(2 < s) & (s < 5)] # More complex boolean; watch operator precedence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look at what happens to the index with a boolean selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[2 < s] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We lose some index entries! \n",
    "\n",
    "But, sometimes, we want to keep the index intact. Here is how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s.where(s > 2) # boolean indexing again, but preserving the original index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas DataFrames\n",
    "\n",
    "> DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"another_s\": [6, 7, 8, 9, 10],\n",
    "                   \"yet another\": [11, None, None, 14, 15]},\n",
    "    index=[\"Lbl1\", \"Lbl2\", \"Lbl3\", \"Lbl4\", \"Lbl5\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.columns # dfs have column indices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrame Data Access\n",
    "\n",
    "You can use the same methods as Series to access and subset DataFrame data.\n",
    "\n",
    "The behaviour is slightly different in some cases, and there some additional functionality that allows you to work with columns.\n",
    "\n",
    "For further details, see "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Accessing Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.another_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'df.yet another' invalid!\n",
    "df[\"yet another\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To select individual rows, use `loc` and `iloc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"Label1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding, Selecting and Reordering Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"s\"] = s # Remeber s?\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"s\", \"another_s\"]] # Subset and reorder columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrame Subsetting\n",
    "\n",
    "You can use boolean expressions as with series, but with different columns too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.s < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"yet another\"] > df.s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working with Missing Data\n",
    "\n",
    "Take another look at the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `NaN` values indicate missing data. \n",
    "\n",
    "Let's briefly review their behavior. We'll look into how to deal with them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[\"yet another na\"] = df[\"yet another\"].isna()\n",
    "df[[\"another_s\", \"yet another\", \"yet another na\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values get propagated in mathematical operations and ignored in aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"another_s\"] + df[\"yet another\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"yet another\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Special Datatypes\n",
    "\n",
    "Pandas has special datatypes for categorical and time data.\n",
    "\n",
    "These datatypes support a number of sophisticated operations.\n",
    "\n",
    "We will see some of them later on, but for reference see the following:\n",
    "- [Categorical data](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html)\n",
    "- [Timeseries data](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Data Wrangling: Loading, Viewing, Cleaning and Enriching Data\n",
    "\n",
    "You can divide data wrangling into three broad activities.\n",
    "1. Inspection - Get familiar with the data and check for potential problems\n",
    "2. Preparation - Create or clean variables, deal with missing values\n",
    "3. Exploration - Compute descriptives statistics and identify basic patterns in the data\n",
    "\n",
    "The ordering above is typical, but not absolute. \n",
    "\n",
    "The different activities often tend to blend together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading and Writing Data\n",
    "\n",
    "`pandas` can import a wide variety of common data formats. \n",
    "\n",
    "Some supported file types include the following.\n",
    "- CSV\n",
    "- Excel\n",
    "- SPSS\n",
    "- STATA\n",
    "\n",
    "For a full list, see the [I/O documentation](https://pandas.pydata.org/docs/reference/io.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can load datasets by calling the appropriate read function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# For this workshop, we will work with the following data\n",
    "load_path = \"federal-candidates-2021-10-20.csv\"\n",
    "\n",
    "# Load the data \n",
    "df = pd.read_csv(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Writing to these formats is also generally supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Let's save a copy of the data to this file\n",
    "save_path = \"federal-candidates-copy.csv\"\n",
    "\n",
    "# To write a dataframe, call the write method for the chosen format\n",
    "df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viewing the Data\n",
    "\n",
    "The first step in analyzing data is to get familiar with it.\n",
    "\n",
    "This way, we get a feel for how much cleaning will be necessary.\n",
    "\n",
    "We also give ourselves a chance to catch anything weird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get our bearings, let's take a look at the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[df.columns[:10]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A good next step is to inspect the data directly and continue to look for\n",
    "anything weird.\n",
    "\n",
    "We can look at the top or bottom of the dataset or to randomly sample rows from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5) # Peek at the first n rows. See anything weird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.tail(5) # Peek at the last n rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5) # Peek at a random sample of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Let's get familiar with some `pandas` data cleaning tools.\n",
    "\n",
    "Our first data cleaning step is to clean up the data types.\n",
    "\n",
    "Sometimes `pandas` fails to assign the right dtype when constructing a variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adjusting Datatypes\n",
    "\n",
    "String data are typically assigned the 'object' type.\n",
    "\n",
    "This is `pandas`'s way of signaling it doesn't really know how to interpret the data.\n",
    "\n",
    "In this dataset, almost all columns with string data represent categorical\n",
    "variables. \n",
    "\n",
    "However, edate is a date, and candidate_name and occupation are a free-form strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "object_cols = df.select_dtypes(include=\"object\").columns\n",
    "print(f\"Object-type columns: {object_cols}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can cast the affected variables to the correct dtypes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Cast edate as a datetime variable\n",
    "df.edate = pd.to_datetime(df.edate, yearfirst=True) \n",
    "\n",
    "# Cast object variables as categorical\n",
    "for col in df.select_dtypes(include=\"object\").columns:\n",
    "    if col not in [\"candidate_name\", \"occupation\"]: \n",
    "        df[col] = df[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Let's check what happened to the object-type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[object_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, let's take a look at the numerical data.\n",
    "\n",
    "Do you notice anything weird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "float_cols = df.select_dtypes(\"float\").columns\n",
    "print(f\"Float columns: {float_cols}\")\n",
    "\n",
    "int_cols = df.select_dtypes(\"int\").columns\n",
    "print(f\"Int columns: {int_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of the four float columns, it is only natural to represent `percent_votes` as a \n",
    "float type variable. \n",
    "\n",
    "For `riding_id`, we are better off using a categorical datatype, even though the \n",
    "values look like integers. \n",
    "\n",
    "Likewise, the `id` column should also be viewed as a\n",
    "categorical. In both cases, the ordering of the values is not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is how we convert the dataype of `riding_id` and `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.riding_id = df.riding_id.astype(\"category\")\n",
    "df.id = df.id.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `birth_year` and `votes` columns should be using an integer dtype.\n",
    "\n",
    "But why were three variables get 'mis-coded' as float in the first place? \n",
    "\n",
    "Perhaps we can get a clue by comparing to correct int variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[[\"birth_year\", \"votes\", \"year\", \"num_candidates\"]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's because the default integer datatype does not support missing\n",
    "values! \n",
    "\n",
    "Luckily, `pandas` has another integer datatype that supports missing values. We\n",
    "can just cast to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.birth_year = df.birth_year.astype(\"Int64\")\n",
    "df.votes = df.votes.astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Don't mix up 'int64' with 'Int64'! In pandas, they are different: \n",
    "- `int64` refers to the regular int type that has no missing value support\n",
    "- `Int64` refers to the to the int type with missing value support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cleaning Categorical Variables\n",
    "\n",
    "Categorical variables tend to require some extra attention, even in the most\n",
    "well-curated datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Take a look at the values of the `gender` variable. \n",
    "\n",
    "Keep in mind what the data's README file says about this variable:\n",
    "> gender is a binary factor variable encoding candidate gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.gender.cat.categories # This is how we access category names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We were told `gender` is supposed to be a binary variable, but we got three values?\n",
    "\n",
    "So what's going on here? \n",
    "\n",
    "To get a better idea, let's tabulate the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look up these cases by subsetting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[df.gender == \"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These candidates are from the most recent election and, if you look them up, you'll find that they both have non-binary gender identity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can recode the data to give those two cases more explicit labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.gender = df.gender.replace({\"2\": \"NB\"})\n",
    "df.gender.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Replace is very powerful and works with other variable types as well!\n",
    "\n",
    "It is particularly handy when you want to collapse multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another adjustment we can make is to rename categories. \n",
    "\n",
    "Why? Because it is confusing and difficult to work with\n",
    "poorly named categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Take a look at the category names in the `censuscategory` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.censuscategory.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`censuscategory` gives candidates' occupation according to the Census Canada taxonomy.\n",
    "\n",
    "These names are precise, but wordy. Let's abbreviate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.censuscategory = df.censuscategory.cat.rename_categories([\n",
    "    \"Business\", \"Health\", \"Management\", \"MP\", \"Science\", \"Resources\", \"Culture\",\n",
    "    \"Social\", \"Manufacturing\", \"Sales\", \"Trades\"\n",
    "])\n",
    "df.censuscategory.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Subsetting the Data\n",
    "\n",
    "Often, we are not interested in the entirety of the data. It then makes sense to subset the data using the techniques we saw above. \n",
    "\n",
    "Here, we'll focus on a subset of the available variables in elections after 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.year > 1990) & (df.type_elxn == \"General\")] # keep general election data from 1990 on\n",
    "df = df[[\"parliament\", \"edate\", \"year\", \"province\", \"riding\", \"id\", \"candidate_name\", \"birth_year\", \n",
    "         \"censuscategory\", \"party_major_group\", \"votes\", \"percent_votes\", \"elected\"]]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "Let's try to identify where we have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df.isna().sum() # .sum() is an aggregation function!\n",
    "na_counts[na_counts > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`pandas` gives us several options to deal with missing values.\n",
    "\n",
    "Our options are basically to do nothing, drop the missing values, or impute them (i.e., fill them in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To keep things simple, let's focus on a single candidate, Elizabeth May (`id == 22644`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "elizabeth_may = df[df.id == 22644][[\"edate\", \"birth_year\", \"censuscategory\"]]\n",
    "elizabeth_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dropping is the simplest solution, but is not ideal because of how much data is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "elizabeth_may.dropna() # Drop all rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elizabeth_may.dropna(axis=1) # Drop columns with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another strategy is to fill in the missing data with some reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "elizabeth_may.birth_year # Original data with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "elizabeth_may.birth_year.fillna(1954) # Fill birth year by fixed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's best not to do manual imputation. We could use some automation instead.\n",
    "\n",
    "A simple automation is to carry existing values forwards or backwards through the data. But, we have to make sure the data is sorted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sorted = elizabeth_may.sort_values(\"edate\") # Make sure data is sorted first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "em_sorted.birth_year.fillna(method=\"bfill\") # Carry birth year back over the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sorted.censuscategory.fillna(method=\"pad\") # Carry occupation forward in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can do more sophisticated imputation as well, using the `interpolate()` method.\n",
    "\n",
    "A detailed discussion of missing values and basic imputation is available in [Working with Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html).\n",
    "\n",
    "Finally, we can combine the imputation functions with techniques discussed in the next section to handle grouped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Enriching the Data with New Variables\n",
    "\n",
    "Once we have sufficiently cleaned our data, we usually want to calculate new variables from it.\n",
    "\n",
    "We can simply calculate the value and add a new variable as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[\"age\"] = df.year - df.birth_year\n",
    "df.age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Grouping\n",
    "\n",
    "Often, we want to calculate statistics by group.\n",
    "\n",
    "For this, we can use `groupby()`, which will automatically split the dataset by group. \n",
    "\n",
    "We can then [aggregate, filter, or transform the data by group](https://pandas.pydata.org/docs/user_guide/groupby.html).\n",
    "\n",
    "As a starting example let's try to find the age of the oldest MP in each parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"parliament\") # Split into groups\n",
    "grouped_age = grouped.age # Select the age variable\n",
    "df[\"oldest_mp_age\"] = grouped_age.transform(max) # Find max age within each group\n",
    "df.oldest_mp_age.describe() # Describe the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To give a more complex example, let's try calculating the margin of victory in each contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby([\"edate\", \"province\", \"riding\"]) # group by contest\n",
    "grouped_pct_votes = grouped.percent_votes # select grouped percent votes variable\n",
    "\n",
    "def compute_margin(data): # define margin computation\n",
    "    if data.size > 1:\n",
    "        runner_up = data.sort_values(ascending=False).iloc[1]\n",
    "        return data - runner_up\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"margin\"] = grouped_pct_votes.transform(compute_margin) # calculate margin\n",
    "df.margin.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method Chaining and Piping\n",
    "\n",
    "Did you notice how tedious it was to calculate the pervious variables? \n",
    "\n",
    "We had to set a lot of variables. \n",
    "\n",
    "We can avoid this using a technique called *method chaining* or *piping*.\n",
    "\n",
    "Here is how it goes for the margin of victory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"margin\"] = (df\n",
    "    .groupby([\"edate\", \"province\", \"riding\"])\n",
    "    .percent_votes\n",
    "    .transform(compute_margin)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This works because each function we call in the sequence is calls a method of the result returned by the previous function.\n",
    "\n",
    "It's a technique is good for grouping together operations into logical chunks. But, try not to abuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens if we want to use piping with functions that are not methods of the result?\n",
    "\n",
    "For instance, what happens if we want to pipe some custom functions?\n",
    "\n",
    "We just use the special method `pipe()`, allows us to pass in arbitrary functions and arguments.\n",
    "\n",
    "- [DataFrame pipe method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html)\n",
    "- Also available for `Series` and grouped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Data Wrangling: Aggregating, Reshaping, Merging and Concatenating Data\n",
    "\n",
    "Often, we need to modify the structure of our dataset.\n",
    "\n",
    "There are a few reasons we might want to do this:\n",
    "- We might want to look at aggregate statistics\n",
    "- Our data might need to be formatted a certain way to facilitate or enable analysis\n",
    "- We might need to incorporate some supplemental data to calculate some variable of interest\n",
    "- Our data may be spread over many files and/or dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aggregating Data\n",
    "\n",
    "We often need to calculate aggregate statistics when trying to provide descriptives for our data. \n",
    "\n",
    "Let's calculate the number of candidates in each election of each party and occupation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (df\n",
    "    .groupby([\"edate\", \"party_major_group\", \"censuscategory\"], as_index=False)\n",
    "    .size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reshaping Data\n",
    "\n",
    "Suppose now that we want the average number of candidates in  each occupation by party.\n",
    "\n",
    "For this, one easy approach is to pivot the party and censuscategory variables to be column indices and then take an average.\n",
    "\n",
    "Refer to [Reshaping and Pivot Tables](https://pandas.pydata.org/docs/user_guide/reshaping.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "count_p = count.pivot(index=\"edate\", columns=[\"party_major_group\", \"censuscategory\"], values=\"size\")\n",
    "count_p.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_p.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can massage the dataset back to its original shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "count_p.unstack().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merging Data\n",
    "\n",
    "To get a handle on merging, let's say we are interested in analyzing candidate occupation by sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sector = pd.DataFrame({\n",
    "            \"sector\": [\"Tertiary\", \"Tertiary\", \"Tertiary\", \"Tertiary\", \"Tertiary\", \"Primary\", \n",
    "                       \"Tertiary\", \"Tertiary\", \"Secondary\", \"Tertiary\", \"Secondary\"],\n",
    "    \"censuscategory\": [\"Business\", \"Health\", \"Management\", \"MP\", \"Science\", \"Resources\", \"Culture\", \n",
    "                       \"Social\", \"Manufacturing\", \"Sales\", \"Trades\"]\n",
    "})\n",
    "sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the merge function we can specify how we want this DataFrame to be combined with the main data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df, sector, on=\"censuscategory\", how=\"left\")\n",
    "merged[[\"year\", \"candidate_name\", \"censuscategory\", \"sector\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look more closely at the code:\n",
    "```python\n",
    "pd.merge(df, sector, on=\"censuscategory\", how=\"left\")\n",
    "```\n",
    "\n",
    "We usually talk about a left and right hand arguments. These are the first two arguments to merge.\n",
    "\n",
    "The `on` argument tells us what variable(s) to merge on. \n",
    "\n",
    "The `how` argument specifies how the data gets merged.\n",
    "\n",
    "To read more see the [guide on database-style joins](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concatenating Data\n",
    "\n",
    "Finally, sometimes our data is spread over multiple dataframes, and we just want to combine it.\n",
    "\n",
    "This is what the concatenation operation does.\n",
    "\n",
    "See [Concatenating Objects](https://pandas.pydata.org/docs/user_guide/merging.html#concatenating-objects) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate counts by election; note iteration over groupby\n",
    "separate_dataframes = [obj for name, obj in count.groupby(\"edate\")]\n",
    "separate_dataframes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat(separate_dataframes) # back together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conlusion\n",
    "\n",
    "We have only scratched the surface of what you can do with `pandas`.\n",
    "\n",
    "Always remember that the documentation is your best friend.\n",
    "\n",
    "Let me leave you with a pointer to a few other libraries;\n",
    "\n",
    "- `numpy` is the standard Python library for mathematical tasks. It is very powerful and you can use it almost seamlessly in conjunction with `pandas`.\n",
    "- If you want to run statistical analyses with your `pandas` data, check out Python's `statsmodels` package.\n",
    "- Finally, if you want to make beautiful graphs, please come back next week, when we will look at `matplotlib`, Python's de facto standard plotting library.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "7bb8eae9c2297506b9894494f89ab8d67c41d115c2354c72d7d438870909f850"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
